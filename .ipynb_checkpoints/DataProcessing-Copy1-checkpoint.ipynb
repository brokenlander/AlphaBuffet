{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272f88a8-c392-44ed-af7b-38a27aeb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "from openai import OpenAI, Client\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91aac8c4-35fe-41bb-8817-cd28903b1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    max_chunk_size: int = 1500  # Default chunk size\n",
    "    min_chunk_size: int = 500   # Minimum chunk size to consider\n",
    "    overlap_size: int = 200     # Number of characters to overlap between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96fe8c8-6a13-4fa8-9e92-0496c673646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main DocumentProcessor class\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Client, config: ChunkConfig):\n",
    "        self.nlp = spacy.load('en_core_web_sm',disable=['tagger','ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks.\"\"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return self._chunk_text(text)\n",
    "\n",
    "    def process_html(self, html_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from HTML and split into chunks.\"\"\"\n",
    "        with open(html_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            text = soup.get_text()\n",
    "            # Normalize whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            text = ' '.join(line for line in lines if line)\n",
    "        return self._chunk_text(text)\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks based on sentence boundaries.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.strip()\n",
    "            sent_length = len(sent_text)\n",
    "            \n",
    "            if current_length + sent_length > self.config.max_chunk_size and current_length >= self.config.min_chunk_size:\n",
    "                # Store current chunk if it's long enough\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                # Start new chunk with overlap from previous chunk\n",
    "                overlap_text = ' '.join(current_chunk[-3:])  # Keep last 3 sentences for context\n",
    "                current_chunk = [overlap_text, sent_text]\n",
    "                current_length = len(overlap_text) + sent_length\n",
    "            else:\n",
    "                current_chunk.append(sent_text)\n",
    "                current_length += sent_length\n",
    "        \n",
    "        # Add the last chunk if it's long enough\n",
    "        if current_length >= self.config.min_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Dict:\n",
    "        \"\"\"Generate a conversation-style QA pair using GPT-4.\"\"\"\n",
    "        # First pass: Check if content is suitable for conversation\n",
    "        validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "- Buffett does not explicitly state a broad lesson. The text must include a clear, stated takeaway that can apply to other cases.\n",
    "\n",
    "If the text mentions a business decision, investment, or deal, it must contain a stated general principle that applies beyond that case. If there is no such general lesson, reject it.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return only \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise.\n",
    "\n",
    "DO NOT RETURN ANY OTHER TEXT APART FROM yes or NO!!!\n",
    "\"\"\"\n",
    "        \n",
    "        validation_response = self.client.chat.completions.create(\n",
    "            model=\"/model\",\n",
    "            messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        if validation_response.choices[0].message.content.strip().lower() != \"yes\":\n",
    "            return None\n",
    "\n",
    "        # Second pass: Generate conversation if content is suitable\n",
    "        conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Generate 1-2 questions that could be asked about this specific content, but ONLY if the text contains clear, direct information to answer them. Then provide my answers in first person, as if I am directly responding to these questions. Use my communication style—plain-spoken, using analogies when helpful, and occasionally humorous.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Important guidelines:\n",
    "- Only generate questions about topics that are explicitly discussed in this text excerpt\n",
    "- Write answers in first person (\"I think...\", \"In my experience...\", \"At Berkshire, we...\")\n",
    "- Use my direct, plain-spoken style\n",
    "- Keep answers focused on what's actually in the text\n",
    "- Return as a JSON string in ShareGPT format:\n",
    "[{{\n",
    "    \"conversations\": [\n",
    "        {{\"role\": \"human\", \"content\": \"question here\"}},\n",
    "        {{\"role\": \"assistant\", \"content\": \"answer here\"}},\n",
    "        {{\"role\": \"human\", \"content\": \"second question\"}},\n",
    "        {{\"role\": \"assistant\", \"content\": \"second answer\"}}\n",
    "    ]\n",
    "}}]\n",
    "\"\"\"\n",
    "        try:\n",
    "            conversation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=1000,\n",
    "                temperature=0.2\n",
    "            )\n",
    "            \n",
    "            # Get the response content and parse it as JSON\n",
    "            response_text = conversation_response.choices[0].message.content\n",
    "            conversation_data = json.loads(response_text)\n",
    "            return conversation_data\n",
    "        except (json.JSONDecodeError, AttributeError, IndexError) as e:\n",
    "            print(f\"Error parsing response for chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def process_directory(input_dir: str, output_dir: str, client: Client, config: ChunkConfig):\n",
    "    \"\"\"Process all PDF and HTML files in a directory and generate training data.\"\"\"\n",
    "    processor = DocumentProcessor(client, config)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for file_path in Path(input_dir).rglob('*'):\n",
    "        if file_path.suffix.lower() in ['.pdf', '.html']:\n",
    "            print(f\"Processing {file_path}\")\n",
    "            output_file = Path(output_dir) / (file_path.stem + '.json')\n",
    "            \n",
    "            chunks = (processor.process_pdf(str(file_path)) if file_path.suffix.lower() == '.pdf' \n",
    "                     else processor.process_html(str(file_path)))\n",
    "            \n",
    "            file_conversations = []\n",
    "            for chunk in chunks:\n",
    "                conversation = processor.generate_conversation(chunk)\n",
    "                if conversation:\n",
    "                    file_conversations.append(conversation)\n",
    "            \n",
    "            # Save conversations for this file\n",
    "            if file_conversations:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\"conversations\": file_conversations}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70106e46-3cda-4eb2-9359-416d87cb53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configuration\n",
    "config = ChunkConfig(\n",
    "    max_chunk_size=1500,\n",
    "    min_chunk_size=500,\n",
    "    overlap_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "779e3261-6548-4d63-8ccf-7327c76298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Client\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://82.150.117.181:8000/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "724b6d70-4c68-43a9-941f-e57c3b26c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 749 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Shareholder Letters/2004ltr.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75e504f2-b73f-474c-931c-20f281e0873f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'288 261 \\nHom eServices .....................................................................................................  130 113 \\nOthe r (net)..........................................................................................................  172 190 \\nLoss from zinc project ........................................................................................       (579)        (46) Earni ngs before corporate i nterest and ta xes......................................................  605 1,076 \\nIntere st, other  than to Berkshi re.........................................................................  (212) (225) Intere st on Be rkshire junior debt ........................................................................  (170) (184) \\nIncome tax..........................................................................................................         (53)      (251) Net ear nings........................................................................................................  $     170 $     416 \\nEarni ngs applicable to Be rkshire*......................................................................  $     237 $     429 \\nDebt owed to others ............................................................................................  10,528 10,296 \\nDebt owed to Berks hire......................................................................................  1,478 1,578 \\n \\n*Includes interest earne d by Berkshire (net of related income taxes) of $110 in 2004 and $118 in 2003. 5\\nInsur ance \\n \\n Since B erkshire purchased National Indem nity (“NIC O”) in 1967, property-casual ty insurance has \\nbeen our core business a nd the propellant of our growth. Insurance has provided a fountain of funds with \\nwhich we’v e acq uired th e secu rities and  businesses th at now give us an ever-wi dening variety of earnin gs \\nstream s. So  in this sectio n, I will b e spending a little ti me telling you how we got where we are. The s ource of our insurance funds is “float,” which is money  that does n’t belong to us but that we \\ntemporarily hold. Most of our fl oat arises because (1) pre miums are pa id upfront though  the servic e we  \\nprovide – insurance protection – i s delivered over a pe riod that usual ly cove rs a y ear and; (2) loss events \\nthat occur toda y do not always resu lt in our immediately p aying claim s, because it sometim es takes many \\nyears for lo sses to  be repo rted (asb estos lo sses wou ld be an example), negotiated  and settled . Th e $20 \\nmillion of float that came with our 1967 purchase has n ow increase d – bot h by way of internal growth and \\nacquisition s – to $46.1 billion. Float is wonderful – if it doesn’t come at a high price. Its co st is d etermined by underwritin g \\nresults, m eaning how th e expen ses an d losses we will ultimatel y pay com pare with  the premiums we h ave \\nreceive d.  Whe n an underwriting profit is achieve d – as has been the case  at Berksh ire in about half of the  \\n38 years we have bee n in the insurance  business – float is better than free. I n such years, we are act ually \\npaid for holding other people’s m oney . For m ost insurers, however, life has been far more di fficult: In \\naggre gate, the property-cas ualty industry al most invariabl y operates at  an underwriting loss . Whe n that \\nloss is large, float becomes expe nsive, sometimes devast atingly so. Insurers have gene rally earned poor returns for a simple reas on: They  sell a com modity-like \\nproduct. Policy forms are st andard, and the product is available from many suppliers, some of whom are \\nmutual co mpanies (“o wned” by policyho lders rath er th an stockholders) with profit g oals that are li mited. Moreover, most insureds don’t care f rom whom they buy. Customers by the millions say “I need some \\nGillette b lades” o r “I’ll h ave a Coke” bu t we wait in  vain for “I’d  like a Natio nal Indemnity policy, p lease.” Consequently, price co mpetition in insurance is u sually fierce. Th ink airline seats. So, you  may ask , how  do Berkshire’s insuran ce operations overc ome the dism al economics of t he \\nindustry an d achieve some measu re of enduring competitiv e advantage? We’v e attack ed th at problem in \\nseveral ways. Let’s lo ok first at N ICO’s stra tegy. When we purchased the company – a sp ecialist in  comme rcial au to and general liab ility insurance \\n– it did not appear t o have  any attributes that would overcome the industry’s chronic troubles. It was not \\nwell-known, had no informational advantage (the com pany has never had an actuary), was not a low-cost \\noperator, and sold through genera l age nts, a method m any people thou ght outdate d.  Neve rtheless, for \\nalmost all o f the past 38 years, NICO h as been a star performer. Ind eed, had we not made this acq uisition, \\nBerk shire would be lucky to be wo rth half of what it is tod ay. What  we’ve had going for us is a managerial mindset  that most insurers find impossi ble to \\nreplicate. Tak e a look at  the facing page. Can you imagine any public com pany e mbracing a busi ness \\nmodel that woul d lead t o the decl ine in revenue that we expe rienced from  198 6 through 1999? That \\ncolossal slide , it shoul d be e mphasized, di d not oc cur b ecause busi ness was unobtaina ble. Many billions o f \\npremium dollars were readily av ailab le to NICO had we only been willing to cut prices. B ut we i nstead \\nconsisten tly priced to make a profit, no t to match  our most optimistic co mpetitor. We never left cu stomers \\n– but they left us. Most Am erican  businesses harbo r an “institutional imperative” th at rej ects ex tended decreases in \\nvolume. What  CEO want s to repo rt to his share holders that not only did business co ntract last year but  that \\nit will  continue to drop ? In insu rance, the  urge to keep writing busi ness is also intensified because the \\nconsequences of foolishly-priced p olicies may not bec ome appare nt for some time. If a n insurer is \\noptimistic in its reserv ing, rep orted earn ings will b e overstated , and years m ay pass before true loss co sts \\nare re veale d (a form  of self-de ception that nearly destroyed GEICO in the early 19 70s). 6\\n \\n \\n \\n \\n \\nPortrait of a Discip lined Underwriter \\nNational Indemnity Company \\n \\n \\n \\n \\n \\n \\n \\nYear  \\n \\n \\nWritten  Premi um \\n(In $ millio ns) \\n \\nNo. of \\nEmployees at \\nYear -End Ratio of \\nOper ating Expenses \\nto \\nWritten  Premi umUnderwriting  Profit \\n(Loss)  as a Per -\\ncentage of  Premiums \\n(Calculated as of  \\nyear e nd 2004)*  \\n \\n1980 ........................... $79.6 372 32.3% 8.2% \\n1981 ........................... 59.9 353 36.1% (.8%) \\n1982 ........................... 52.5 323 36.7% (15.3%) \\n1983 ........................... 58.2 308 35.6% (18.7%) \\n1984 ........................... 62.2 342 35.5% (17.0%) \\n1985 ........................... 160.7 380 28.0% 1.9% \\n1986 ........................... 366.2 403 25.9% 30.7% \\n1987 ........................... 232.3 368 29.5% 27.3% \\n1988 ........................... 139.9 347 31.7% 24.8% \\n1989 ........................... 98.4 320 35.9% 14.8% \\n1990 ........................... 87.8 289 37.4% 7.0% \\n1991 ........................... 88.3 284 35.7% 13.0% \\n1992 ........................... 82.7 277 37.9% 5.2% \\n1993 ........................... 86.8 279 36.1% 11.3% \\n1994 ........................... 85.9 263 34.6% 4.6% \\n1995 ........................... 78.0 258 36.6% 9.2% \\n1996 ........................... 74.0 243 36.5% 6.8% \\n1997 ........................... 65.3 240 40.4% 6.2% \\n1998 ........................... 56.8 231 40.4% 9.4% \\n1999 ........................... 54.5 222 41.2% 4.5% \\n2000 ........................... 68.1 230 38.4% 2.9% \\n2001 ........................... 161.3 254 28.8% (11.6%) \\n2002 ........................... 343.5 313 24.0% 16.8% \\n2003 ........................... 594.5 337 22.2% 18.1% \\n2004 ........................... 605.6 340 22.5% 5.1% \\n \\n*It takes a l ong time to learn the tr ue profitability of any given year. First, m any clai ms are receive d after \\nthe end of t he year, an d we must esti mate how many of these t here will b e and what they will co st. (In \\ninsurance jargon, these claim s are te rmed IB NR – incurred but not reported.) Sec ond, claims often take \\nyears, or eve n decade s, to settle, which m eans there  can be  many su rprises  along t he way. For these reason s, the results in  this column simply rep resent o ur best esti mate at the end of  2004 as to  how \\nwe have don e in prior year s.  Pro fit margins for the year s thro ugh 1999 are probably close to corr ect \\nbecause these years are “m ature,” in the se nse that they have fe w clai ms still outstanding. The m ore recent \\nthe year, th e more guesswork  is invo lved. In particular, th e resu lts sho wn for 200 3 and 2004 are apt to \\nchange sign ifican tly. 7\\n Finally, there is a fear factor at work , in that a shri nking business us ually leads t o layoffs. To  \\navoid pink slips, em ployees will ration alize in adequate pricin g, tellin g themselves th at poorly-priced \\nbusiness m ust be tolerated in order to keep t he organization intact and t he distribution system happy . If t his \\ncourse isn’t follo wed, th ese e mployees will arg ue, the company will  not particip ate in  the reco very th at \\nthey inva riably  feel is just around t he corner. To com bat employees’ nat ural tendency  to save t heir own ski ns, we ha ve al ways prom ised \\nNICO’s workforce that no one will be fired because  of declining volu me, howeve r severe the c ontrac tion. (This is  not Donald T rump’s sort of place.) NICO is not la bor-i ntensi ve, and, as t he tabl e sugge sts, ca n live \\nwith excess overhead. It can ’t live, howev er, with  underpriced business a nd the breakdown in underwriting \\ndiscipline t hat accom panies it. An in suranc e orga nization that doesn’t ca re deeply a bout unde rwriting at a \\nprofit this year is un likely to  care next year ei ther. Naturally, a business th at fo llows a n o-layo ff policy m ust be especia lly careful  to avoid \\noverstaffing  when times are go od. Th irty years ag o Tom Murphy, then CEO of Cap Cities, d rove this point \\nhome to me with a hypothetical tale about  an em ployee who asked his boss for permission to hire an  \\nassistant. The employee assu med that adding $20,000 to the annual payroll would be inconsequential. But \\nhis boss told him the proposal should be evaluated as a $3  million decision, given that an  additional person \\nwould probably cost  at least that amount over his lifetime, fact oring in raises, be nefits and other expenses \\n(more pe ople, more toilet paper). And unless the com pany fell on very hard times, the em ployee added \\nwould be unlikely to be dismissed,  however marginal his contribution to the business. It takes re al fortitude – em bedde d deep within a company’s cul ture – to operate as NICO does. Anyone exam ining the  table can sca n the years from  1986 to 1999 quickly. Bu t living day after day with \\ndwindling volume – while co mpetitors are b oasting of growth and reaping Wall Street ’s app lause –  is an \\nexperience few managers can tolerate.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192517d8-7192-40ba-ada6-1ddb5bb704ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample conversation:\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    conversation = processor.generate_conversation(chunks[16])\n",
    "    print(\"Sample conversation:\")\n",
    "    print(json.dumps(conversation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5023334f-222d-49e3-8dd1-7a656d11fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Shareholder Letters/2001pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2002pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2003ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2004ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2005ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2006ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2007ltr.pdf\n",
      "Error parsing response for chunk: 2 Berkshire’s Corporate Performance vs. the S&P 500 \n",
      "   Annual Percentage Change  \n",
      "  in Per-Share in...\n",
      "Error details: Expecting ',' delimiter: line 6 column 591 (char 1745)\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2008ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2009ltr.pdf\n",
      "Error parsing response for chunk: 2 5 1 8 6\n",
      "Operating earnings before corporate interest and taxes ........................... 1,846 2...\n",
      "Error details: Expecting ',' delimiter: line 6 column 586 (char 1365)\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2010ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2011ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2012ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2013ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2014ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2015ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2016ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2017ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2018ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2019ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2020ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2021ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2022ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2023ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1998pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1999pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2000pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/.ipynb_checkpoints/2000pdf-checkpoint.pdf\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Shareholder Letters/\"\n",
    "output_directory = \"Dataset/Processed/Shareholder Letters/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f229c-917f-448e-8db9-6c6075a3a6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ff69a-a12e-4f65-a3b9-b84614ba0881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
