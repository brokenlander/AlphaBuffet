{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "272f88a8-c392-44ed-af7b-38a27aeb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import spacy\n",
    "import re\n",
    "import fitz\n",
    "from openai import OpenAI, Client\n",
    "from anthropic import Anthropic\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a73cc985-0cef-4594-862f-5d4fef7505eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20426343-8d93-45e2-a994-be88aa55e69c",
   "metadata": {},
   "source": [
    "# Initialize OpenAI Cleint\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://82.150.117.181:8000/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b74b00ee-f8ac-4ac6-b013-840fa78c68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To swap to vllm:\n",
    "#  1.  \"claude-3-5-sonnet-latest\" --> \"/model\"\n",
    "#  2.  .content[0].text --> choices[0].message.content \n",
    "#  3.  client.messages --> client.chat.completions\n",
    "\n",
    "class SplittingStrategy(Enum):\n",
    "    NUMBERED_SECTIONS = \"numbered_sections\"\n",
    "    SENTENCE_OVERLAP = \"sentence_overlap\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking and processing.\"\"\"\n",
    "    max_chunk_size: int = 1600\n",
    "    min_chunk_size: int = 500  # Only used for sentence overlap strategy\n",
    "    overlap_sentences: int = 2  # Only used for sentence overlap strategy\n",
    "    strategy: SplittingStrategy = SplittingStrategy.NUMBERED_SECTIONS\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Anthropic, config: ChunkConfig):\n",
    "        \"\"\"Initialize the document processor.\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['tagger', 'ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text while preserving important newlines.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        lines = [' '.join(line.split()) for line in lines if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks.\"\"\"\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        clean_text = self.preprocess_text(text)\n",
    "        return self._chunk_text(clean_text)\n",
    "\n",
    "    def process_txt(self, txt_path: str) -> List[str]:\n",
    "        \"\"\"Read text file and split into chunks.\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            return self._chunk_text(clean_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text file {txt_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _split_by_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text using sentence overlap strategy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [Sentence(text=sent.text.strip(), length=len(sent.text.strip())) \n",
    "                    for sent in doc.sents if sent.text.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence.length\n",
    "            \n",
    "            if current_length >= self.config.max_chunk_size and len(current_chunk) > self.config.overlap_sentences:\n",
    "                if current_length >= self.config.min_chunk_size:\n",
    "                    chunks.append(' '.join(s.text for s in current_chunk))\n",
    "                    overlap_sentences = current_chunk[-self.config.overlap_sentences:]\n",
    "                    current_chunk = overlap_sentences.copy()\n",
    "                    current_length = sum(s.length for s in current_chunk)\n",
    "\n",
    "        if current_length >= self.config.min_chunk_size:\n",
    "            chunks.append(' '.join(s.text for s in current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def _split_by_numbered_sections(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text based on numbered sections.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        initial_chunks = []\n",
    "        current_chunk = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if re.match(r'^\\d+\\.', line.strip()):\n",
    "                if current_chunk:\n",
    "                    initial_chunks.append('\\n'.join(current_chunk))\n",
    "                current_chunk = [line]\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "        \n",
    "        if current_chunk:\n",
    "            initial_chunks.append('\\n'.join(current_chunk))\n",
    "        \n",
    "        # Handle chunks that are too large\n",
    "        final_chunks = []\n",
    "        for chunk in initial_chunks:\n",
    "            if len(chunk) > self.config.max_chunk_size:\n",
    "                doc = self.nlp(chunk)\n",
    "                sentences = list(doc.sents)\n",
    "                mid_point = len(sentences) // 2\n",
    "                first_half = ' '.join(sent.text.strip() for sent in sentences[:mid_point])\n",
    "                second_half = ' '.join(sent.text.strip() for sent in sentences[mid_point:])\n",
    "                final_chunks.extend([first_half, second_half])\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "                \n",
    "        return final_chunks\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text using the configured strategy.\"\"\"\n",
    "        if self.config.strategy == SplittingStrategy.NUMBERED_SECTIONS:\n",
    "            return self._split_by_numbered_sections(text)\n",
    "        else:\n",
    "            return self._split_by_sentences(text)\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate conversation from chunk with validation.\"\"\"\n",
    "        try:\n",
    "            validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return ONLY \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise. I WANT A SINGLE YES or NO!!\"\"\"\n",
    "            \n",
    "            validation_response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-latest\",\n",
    "                messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            if validation_response.content[0].text.strip().lower() != \"yes\":\n",
    "                return None\n",
    "\n",
    "            conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Generate 2 questions about the key themes in this content. Then, provide answers using only my exact words and ideas from this text, but restructure them into my characteristic Q&A style like at the annual meetings. Maintain my plain-spoken voice while elaborating on the concepts present in the text.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Guidelines:\n",
    "- Questions should focus on the main principles or ideas explicitly discussed in the text\n",
    "- Answers should:\n",
    "  * Start with the core information from the text\n",
    "  * Elaborate on these specific points in my conversational style\n",
    "  * Use any analogies or examples that appear in the text\n",
    "  * Maintain my natural speaking rhythm and tone\n",
    "  * Focus on clarity and directness\n",
    "- Write answers in first person\n",
    "- Aim for detailed responses (200+ words) while staying true to the source material\n",
    "\n",
    "Return as a properly formatted JSON string without any control characters or extra whitespace in ShareGPT format:\n",
    "{{\"conversations\": [\n",
    "  [\n",
    "    {{\"from\": \"human\", \"value\": \"question here\"}},\n",
    "    {{\"from\": \"gpt\", \"value\": \"answer here\"}}\n",
    "  ],\n",
    "  [\n",
    "    {{\"from\": \"human\", \"value\": \"second question\"}},\n",
    "    {{\"from\": \"gpt\", \"value\": \"second answer\"}}\n",
    "  ]\n",
    "]}}\n",
    "\n",
    "Important: Ensure the response is valid JSON with no control characters, and all whitespace in values is single spaces only.\"\"\"\n",
    "\n",
    "            conversation_response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-latest\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.4\n",
    "            )\n",
    "            \n",
    "            response_text = conversation_response.content[0].text.strip()\n",
    "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            try:\n",
    "                conversation_data = json.loads(response_text)\n",
    "                return conversation_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON parsing error: {str(e)}\")\n",
    "                print(f\"Response text: {response_text[:200]}...\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50b9c794-61d1-4575-838f-e224aa707cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ChunkConfig(\n",
    "    max_chunk_size=1600,\n",
    "    min_chunk_size=500,\n",
    "    overlap_sentences=2,\n",
    "    strategy=SplittingStrategy.SENTENCE_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d51065e-6c80-4f61-a47b-aaded0133f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 378 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Lessons for Corporate America/Lessons-for-Corporate-America.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d76c7c-1ffa-4eea-a5f4-1475722a700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunks:\n",
    "    conversation = processor.generate_conversation(chunks[23])\n",
    "    print(\"Sample conversation:\")\n",
    "    print(json.dumps(conversation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4955239-6499-4159-8bda-2fc54b8f7945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Shareholder Letters/1977.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1977.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1978.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1978.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1979.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1979.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1980.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1980.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1981.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1981.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1982.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1982.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1983.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1983.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1984.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1984.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1985.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1985.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1986.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1986.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1987.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1987.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1988.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1988.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1989.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1989.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1990.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1990.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1991.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1991.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1992.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1992.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1993.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1993.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1994.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1994.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1995.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1995.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1996.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1996.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1997.txt\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1997.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1998pdf.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1998pdf.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1999pdf.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/1999pdf.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2000pdf.pdf\n",
      "JSON parsing error: Expecting ',' delimiter: line 10 column 2 (char 2402)\n",
      "Response text: {\n",
      "\"conversations\": [\n",
      "[\n",
      "{\"from\": \"human\", \"value\": \"You've often spoken about the impact of Ben Graham on your investment approach. Could you explain how his teachings transformed your results?\"},\n",
      "{\"fr...\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2000pdf.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2001pdf.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2001pdf.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2002pdf.pdf\n",
      "Error processing chunk: It will be a great many years before\n",
      "we are totally out of this operation (though we reduce our expo...\n",
      "Error details: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Output blocked by content filtering policy'}}\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2002pdf.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2003ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2003ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2004ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2004ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2005ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2005ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2006ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2006ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2007ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2007ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2008ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2008ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2009ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2009ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2010ltr.pdf\n",
      "Error processing chunk: In most years, industry premiums have been inadequate to cover claims plus expenses. Consequently, t...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2010ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2011ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2011ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2012ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2012ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2013ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2013ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2014ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2014ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2015ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2015ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2016ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2016ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2017ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2017ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2018ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2018ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2019ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2019ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2020ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2020ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2021ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2021ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2022ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2022ltr.json\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2023ltr.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Shareholder Letters/2023ltr.json\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Shareholder Letters/\"\n",
    "output_directory = \"Dataset/Processed/Shareholder Letters/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d0d62b1-b3a7-4d52-83e0-7fa4f9fd02b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Lessons for Corporate America/Lessons-for-Corporate-America.pdf\n",
      "Successfully saved conversations to Dataset/Processed/Lessons for Corporate America/Lessons-for-Corporate-America.json\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"Dataset/Unprocessed/Lessons for Corporate America/\"\n",
    "output_directory = \"Dataset/Processed/Lessons for Corporate America/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c781c7c-d0d0-4ede-bbce-8b8cd77a4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ChunkConfig(\n",
    "    max_chunk_size=1600,\n",
    "    strategy=SplittingStrategy.NUMBERED_SECTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5f5efc2-3bbb-440a-88e1-76a5c69d267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3796 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2710532d-1d4d-4f80-bd43-0cae69dd1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.pdf\n",
      "Error processing chunk: 18. Unrealistic investment expectations for pension funds\n",
      "WARREN BUFFETT: Zone 8.\n",
      "AUDIENCE MEMBER: M...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Error processing chunk: 9. Buffett (D) and Munger (R) both endorse Social Security\n",
      "WARREN BUFFETT: Number 3. (Laughter)\n",
      "AUDI...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Error processing chunk: And I think you’ve seen that over the last two years and we’re seeing it month by month. I would say...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Error processing chunk: 11. Why See’s Candies does well in inflationary times\n",
      "WARREN BUFFETT: With those modest statements, ...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Error processing chunk: Charlie?\n",
      "CHARLIE MUNGER: Well, but like our insurance operations, our capital intensive railroad\n",
      "bus...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Error processing chunk: 12. “May you live until the A stock splits”\n",
      "WARREN BUFFETT: OK. Area 1 again.\n",
      "AUDIENCE MEMBER: I’m M...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Error processing chunk: 13. Buffett’s best deal: hiring Ajit Jain\n",
      "WARREN BUFFETT: Andrew?\n",
      "ANDREW ROSS SORKIN: This question ...\n",
      "Error details: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "Successfully saved conversations to Dataset/Processed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.json\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Meeting Transcripts/\"\n",
    "output_directory = \"Dataset/Processed/Meeting Transcripts/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3eb19d26-0553-4f77-a622-5b105af971fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(input_directory, output_path):\n",
    "    \"\"\"\n",
    "    Merge all JSON files in the specified directory into a single JSON file\n",
    "    maintaining the nested 'conversations' structure.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing JSON files\n",
    "        output_path (str): Full path (including filename) for the output merged JSON file\n",
    "    \"\"\"\n",
    "    # Initialize the merged structure\n",
    "    merged_data = {\n",
    "        \"conversations\": []\n",
    "    }\n",
    "    \n",
    "    # Convert string paths to Path objects\n",
    "    directory = Path(input_directory)\n",
    "    output = Path(output_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all JSON files in the directory\n",
    "    for file_path in directory.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Check if the file has the expected structure\n",
    "                if \"conversations\" in data:\n",
    "                    # Extend the conversations list with the new data\n",
    "                    merged_data[\"conversations\"].extend(data[\"conversations\"])\n",
    "                else:\n",
    "                    print(f\"Warning: File {file_path} does not have the expected structure\")\n",
    "                    \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not parse JSON from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Write the merged data to the specified output path\n",
    "    try:\n",
    "        with open(output, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Successfully created merged file at: {output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing merged file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab7d5aa7-9e56-4010-9536-060b09498b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created merged file at: Dataset/Processed/Shareholder Letters/Letters.json\n"
     ]
    }
   ],
   "source": [
    "merge_json_files('Dataset/Processed/Shareholder Letters/', 'Dataset/Processed/Shareholder Letters/Letters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c26a861-4cf8-4be9-aca1-5bf745ed857c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset/Processed/Ground Truth/Transcripts.json'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('Dataset/Processed/Ground Truth', exist_ok=True)\n",
    "shutil.copy2('Dataset/Processed/Shareholder Letters/Letters.json', 'Dataset/Processed/Ground Truth/Letters.json')\n",
    "shutil.copy2('Dataset/Processed/Lessons for Corporate America/Lessons-for-Corporate-America.json', 'Dataset/Processed/Ground Truth/Lessons.json')\n",
    "shutil.copy2('Dataset/Processed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.json', 'Dataset/Processed/Ground Truth/Transcripts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8939a8d4-71de-418a-8191-dc51f4978450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created merged file at: Dataset/Processed/Ground Truth/dataset_combined.json\n"
     ]
    }
   ],
   "source": [
    "merge_json_files('Dataset/Processed/Ground Truth/', 'Dataset/Processed/Ground Truth/dataset_combined.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219be00-617e-403e-92a9-1c1904845509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
