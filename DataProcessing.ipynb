{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "272f88a8-c392-44ed-af7b-38a27aeb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import spacy\n",
    "import re\n",
    "import fitz\n",
    "from openai import OpenAI, Client\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96fe8c8-6a13-4fa8-9e92-0496c673646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    \"\"\"Represents a single sentence with its text and length.\"\"\"\n",
    "    text: str\n",
    "    length: int\n",
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking and processing.\"\"\"\n",
    "    max_chunk_size: int = 1500\n",
    "    min_chunk_size: int = 500\n",
    "    overlap_sentences: int = 2\n",
    "    \n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Client, config: ChunkConfig):\n",
    "        \"\"\"Initialize the document processor with improved chunking capabilities.\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['tagger', 'ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text before processing.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        # Normalize line endings\n",
    "        text = text.replace('\\n', ' ')\n",
    "        return text\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks with improved handling.\"\"\"\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        # Preprocess the extracted text\n",
    "        clean_text = self.preprocess_text(text)\n",
    "        return self._chunk_text(clean_text)\n",
    "\n",
    "\n",
    "    def process_txt(self, txt_path: str) -> List[str]:\n",
    "        \"\"\"Read text file and split into chunks.\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Preprocess the text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            return self._chunk_text(clean_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text file {txt_path}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def create_sentence_objects(self, doc) -> List[Sentence]:\n",
    "        \"\"\"Convert spaCy doc into list of Sentence objects.\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            text = sent.text.strip()\n",
    "            if text:  # Only include non-empty sentences\n",
    "                sentences.append(Sentence(text=text, length=len(text)))\n",
    "        return sentences\n",
    "\n",
    "    def _chunk_sentences(self, sentences: List[Sentence]) -> List[List[Sentence]]:\n",
    "        \"\"\"Split sentences into chunks while maintaining proper overlap.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # Always add the current sentence\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence.length\n",
    "            \n",
    "            # Check if we should create a new chunk\n",
    "            if current_length >= self.config.max_chunk_size and len(current_chunk) > self.config.overlap_sentences:\n",
    "                # Only create chunk if it meets minimum size\n",
    "                if current_length >= self.config.min_chunk_size:\n",
    "                    chunks.append(current_chunk)\n",
    "                    \n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_sentences = current_chunk[-self.config.overlap_sentences:]\n",
    "                    current_chunk = overlap_sentences.copy()\n",
    "                    current_length = sum(s.length for s in current_chunk)\n",
    "\n",
    "        # Add the last chunk if it meets minimum size\n",
    "        if current_length >= self.config.min_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Enhanced text chunking with better overlap handling.\"\"\"\n",
    "        # Create spaCy doc and convert to sentence objects\n",
    "        doc = self.nlp(text)\n",
    "        sentences = self.create_sentence_objects(doc)\n",
    "        \n",
    "        # Create chunks of sentences\n",
    "        sentence_chunks = self._chunk_sentences(sentences)\n",
    "        \n",
    "        # Convert chunks of sentences back to text\n",
    "        text_chunks = []\n",
    "        for chunk in sentence_chunks:\n",
    "            chunk_text = ' '.join(sentence.text for sentence in chunk)\n",
    "            text_chunks.append(chunk_text)\n",
    "            \n",
    "        return text_chunks\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate conversation from chunk with validation.\"\"\"\n",
    "        # Validation prompt remains unchanged\n",
    "        validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "- Buffett does not explicitly state a broad lesson. The text must include a clear, stated takeaway that can apply to other cases.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return only \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise.\n",
    "\"\"\"\n",
    "        try:\n",
    "            validation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            if validation_response.choices[0].message.content.strip().lower() != \"yes\":\n",
    "                return None\n",
    "\n",
    "            # Generate conversation prompt remains unchanged\n",
    "            conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Generate 1-2 questions that could be asked about this specific content, but ONLY if the text contains clear, direct information to answer them. Then provide my answers in first person, as if I am directly responding to these questions. Use my communication style—plain-spoken, using analogies when helpful, and occasionally humorous.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Important guidelines:\n",
    "- Only generate questions about topics that are explicitly discussed in this text excerpt\n",
    "- Write answers in first person\n",
    "- Use my direct, plain-spoken style.\n",
    "- Keep answers focused on what's actually in the text\n",
    "- Return as a JSON string in ShareGPT format:\n",
    "[{{\"conversations\": [\n",
    "    {{\"role\": \"human\", \"content\": \"question here\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"answer here\"}},\n",
    "    {{\"role\": \"human\", \"content\": \"second question\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"second answer\"}}\n",
    "]}}]\"\"\"\n",
    "\n",
    "            conversation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.4\n",
    "            )\n",
    "            \n",
    "            response_text = conversation_response.choices[0].message.content\n",
    "            return json.loads(response_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def process_directory(input_dir: str, output_dir: str, client: Client, config: ChunkConfig):\n",
    "    \"\"\"Process all PDF and TXT files in a directory and generate training data.\"\"\"\n",
    "    processor = DocumentProcessor(client, config)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Walk through directory, skipping hidden directories\n",
    "    for file_path in Path(input_dir).rglob('*'):\n",
    "        # Skip hidden directories and their contents\n",
    "        if any(part.startswith('.') for part in file_path.parts):\n",
    "            continue\n",
    "            \n",
    "        if file_path.suffix.lower() in ['.pdf', '.txt']:\n",
    "            print(f\"Processing {file_path}\")\n",
    "            output_file = Path(output_dir) / (file_path.stem + '.json')\n",
    "            \n",
    "            # Process based on file type\n",
    "            if file_path.suffix.lower() == '.pdf':\n",
    "                chunks = processor.process_pdf(str(file_path))\n",
    "            else:  # .txt file\n",
    "                chunks = processor.process_txt(str(file_path))\n",
    "            \n",
    "            file_conversations = []\n",
    "            for chunk in chunks:\n",
    "                conversation = processor.generate_conversation(chunk)\n",
    "                if conversation:\n",
    "                    file_conversations.append(conversation)\n",
    "            \n",
    "            # Save conversations for this file\n",
    "            if file_conversations:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\"conversations\": file_conversations}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70106e46-3cda-4eb2-9359-416d87cb53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configuration\n",
    "config = ChunkConfig(\n",
    "    max_chunk_size=1500,    # Maximum characters per chunk\n",
    "    min_chunk_size=500,     # Minimum characters per chunk\n",
    "    overlap_sentences=2      # Number of sentences to overlap (changed from overlap_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779e3261-6548-4d63-8ccf-7327c76298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Client\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://82.150.117.181:8000/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724b6d70-4c68-43a9-941f-e57c3b26c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infra/miniforge3/envs/unsloth/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 406 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Lessons for Corporate America/Lessons-for-Corporate-America.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e504f2-b73f-474c-931c-20f281e0873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997] THE ESSAYS OF WARREN BUFFETT 15 Buffett learned the art of investing from Ben Graham as a graduate student at Columbia Business School in the 1950s and later working at Graham-Newman. In a number of classic works, including The Intelligent Investor, Graham introduced some of the most profound investment wisdom in history. It rejects a prevalent but mistaken mind-set that equates price with value. On the con- trary, Graham held that price is what you pay and value is what you get. These two things are rarely identical, but most people rarely notice any difference. One of Graham's most profound contributions is a character who lives on Wall Street, Mr. Market. He is your hypothetical business partner who is daily willing to buy your interest in a busi- ness or sell you his at prevailing market prices. Mr. Market is moody, prone to manic swings from joy to despair. Sometimes he offers prices way higher than value; sometimes he offers prices way lower than value. The more manic-depressive he is, the greater the spread between price and value, and therefore the greater the in- vestment opportunities he offers. Buffett reintroduces Mr. Market, emphasizing how valuable Graham's allegory of the overall market is for disciplined investment knitting-even though Mr. Market would be unrecognizable to modern finance theorists. Another leading prudential legacy from Graham is his margin- of-safety principle. This principle holds that one should not make an investment in a security unless there is a sufficient basis for be- lieving that the price being paid is substantially lower than the value being delivered.\n"
     ]
    }
   ],
   "source": [
    "print(chunks[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192517d8-7192-40ba-ada6-1ddb5bb704ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample conversation:\n",
      "[\n",
      "  {\n",
      "    \"conversations\": [\n",
      "      {\n",
      "        \"role\": \"human\",\n",
      "        \"content\": \"What is the main difference between price and value according to Ben Graham's investment wisdom?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Well, as I learned from Ben, price is what you pay, and value is what you get. These two aren't always the same, but a lot of people don't notice the difference. It's a crucial distinction, though, and one that's served me well over the years.\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"human\",\n",
      "        \"content\": \"Can you describe Mr. Market, a character introduced by Ben Graham, and his significance in investing?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Mr. Market is a great allegory. He's like a business partner who's willing to buy or sell his interest in a business at the prevailing market price every day. The thing is, Mr. Market can be quite moody, swinging from extreme optimism to deep pessimism. This moodiness creates opportunities for investors like me, as it can drive prices far away from true value. By understanding Mr. Market's behavior, I can make more informed investment decisions.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    conversation = processor.generate_conversation(chunks[23])\n",
    "    print(\"Sample conversation:\")\n",
    "    print(json.dumps(conversation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50f229c-917f-448e-8db9-6c6075a3a6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Shareholder Letters/1977.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1978.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1979.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1980.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1981.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1982.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1983.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1984.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1985.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1986.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1987.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1989.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1988.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1990.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1991.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1992.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1993.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1994.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1995.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1996.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1997.txt\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2023ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2021ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2022ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2020ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2019ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2018ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2017ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2016ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2015ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2014ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2013ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2012ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2011ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2010ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2009ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2008ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2007ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2006ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2005ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2004ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2003ltr.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2002pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2001pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/2000pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1999pdf.pdf\n",
      "Processing Dataset/Unprocessed/Shareholder Letters/1998pdf.pdf\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Shareholder Letters/\"\n",
    "output_directory = \"Dataset/Processed/Shareholder Letters/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90983a70-f5cb-4139-9d69-20e3322a4e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Lessons for Corporate America/Lessons-for-Corporate-America.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infra/miniforge3/envs/unsloth/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Lessons for Corporate America/\"\n",
    "output_directory = \"Dataset/Processed/Lessons for Corporate America/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818df5f2-e08e-446a-b0ad-165b82f177fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    \"\"\"Represents a single sentence with its text and length.\"\"\"\n",
    "    text: str\n",
    "    length: int\n",
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking and processing.\"\"\"\n",
    "    max_chunk_size: int = 1600\n",
    "    overlap_sentences: int = 1\n",
    "    \n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Client, config: ChunkConfig):\n",
    "        \"\"\"Initialize the document processor.\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['tagger', 'ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text while preserving important newlines.\"\"\"\n",
    "        # Split into lines\n",
    "        lines = text.split('\\n')\n",
    "        # Remove empty lines and excessive whitespace within lines\n",
    "        lines = [' '.join(line.split()) for line in lines if line.strip()]\n",
    "        # Rejoin with newlines\n",
    "        return '\\n'.join(lines)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks.\"\"\"\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        # Preprocess the extracted text\n",
    "        clean_text = self.preprocess_text(text)\n",
    "        return self._chunk_text(clean_text)\n",
    "\n",
    "    def process_txt(self, txt_path: str) -> List[str]:\n",
    "        \"\"\"Read text file and split into chunks.\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Preprocess the text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            return self._chunk_text(clean_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text file {txt_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _split_large_chunk(self, text: str) -> List[str]:\n",
    "        \"\"\"Split a large chunk of text into roughly equal parts at sentence boundaries.\"\"\"\n",
    "        # Use spaCy to split into sentences\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return [text]\n",
    "            \n",
    "        # Calculate target split point\n",
    "        total_length = len(text)\n",
    "        target_length = total_length // 2\n",
    "        \n",
    "        # Find best split point\n",
    "        current_length = 0\n",
    "        best_split_idx = 0\n",
    "        \n",
    "        for i, sent in enumerate(sentences):\n",
    "            current_length += len(sent.text)\n",
    "            if current_length >= target_length:\n",
    "                best_split_idx = i\n",
    "                break\n",
    "        \n",
    "        # Create the two chunks\n",
    "        first_chunk = ' '.join(sent.text.strip() for sent in sentences[:best_split_idx + 1])\n",
    "        second_chunk = ' '.join(sent.text.strip() for sent in sentences[best_split_idx + 1:])\n",
    "        \n",
    "        return [first_chunk, second_chunk]\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks based on numbered sections and size limits.\"\"\"\n",
    "        # Split text into lines\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        initial_chunks = []\n",
    "        current_chunk = []\n",
    "        \n",
    "        # First split by numbered sections\n",
    "        for line in lines:\n",
    "            # Check if line starts with a number followed by a dot\n",
    "            if re.match(r'^\\d+\\.', line.strip()):\n",
    "                # If we have accumulated lines in the current chunk, save it\n",
    "                if current_chunk:\n",
    "                    initial_chunks.append('\\n'.join(current_chunk))\n",
    "                # Start a new chunk with the current line\n",
    "                current_chunk = [line]\n",
    "            else:\n",
    "                # Add line to current chunk\n",
    "                current_chunk.append(line)\n",
    "        \n",
    "        # Add the last chunk if it exists\n",
    "        if current_chunk:\n",
    "            initial_chunks.append('\\n'.join(current_chunk))\n",
    "        \n",
    "        # Further split chunks that are too large\n",
    "        final_chunks = []\n",
    "        for chunk in initial_chunks:\n",
    "            if len(chunk) > self.config.max_chunk_size:\n",
    "                split_chunks = self._split_large_chunk(chunk)\n",
    "                final_chunks.extend(split_chunks)\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "            \n",
    "        return final_chunks\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate conversation from chunk with validation.\"\"\"\n",
    "        # Validation prompt remains unchanged\n",
    "        validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "- Buffett does not explicitly state a broad lesson. The text must include a clear, stated takeaway that can apply to other cases.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return only \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise.\n",
    "\"\"\"\n",
    "        try:\n",
    "            validation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            if validation_response.choices[0].message.content.strip().lower() != \"yes\":\n",
    "                return None\n",
    "\n",
    "            conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Generate 1-2 questions that could be asked about this specific content, but ONLY if the text contains clear, direct information to answer them. Then provide my answers in first person, as if I am directly responding to these questions. Use my communication style—plain-spoken, using analogies when helpful, and occasionally humorous.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Important guidelines:\n",
    "- Only generate questions about topics that are explicitly discussed in this text excerpt\n",
    "- Write answers in first person\n",
    "- Use my direct, plain-spoken style.\n",
    "- Keep answers focused on what's actually in the text\n",
    "- Return as a JSON string in ShareGPT format:\n",
    "[{{\"conversations\": [\n",
    "    {{\"role\": \"human\", \"content\": \"question here\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"answer here\"}},\n",
    "    {{\"role\": \"human\", \"content\": \"second question\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"second answer\"}}\n",
    "]}}]\"\"\"\n",
    "\n",
    "            conversation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.4\n",
    "            )\n",
    "            \n",
    "            response_text = conversation_response.choices[0].message.content\n",
    "            return json.loads(response_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d4ff69a-a12e-4f65-a3b9-b84614ba0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ChunkConfig()\n",
    "processor = DocumentProcessor(client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37969412-d94f-4d07-8be0-6c8427db5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3796\n"
     ]
    }
   ],
   "source": [
    "# Process single PDF\n",
    "pdf_path = \"Dataset/Unprocessed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.pdf\"\n",
    "chunks = processor.process_pdf(pdf_path)\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b91761-f136-48eb-b0bc-0156f00b6001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2. Opening remarks\\nWARREN BUFFETT: Got a lot of people to thank, starting off with Jimmy. Wonderful. We hid him out — came in last night kind of late and we — to be sure it was a surprise, we\\nstashed him away over at the Hilton, and I just want to say thanks to him. We both got the commercial gene, but unfortunately, he got the singing gene. I got this voice\\nyou’re hearing. We — the movie, as we mentioned, we get a lot of help from a lot of people. They all do it just\\nfor the fun of it. I particularly want to thank Andy Heyward of DIC who did that cartoon. He’s done them now for\\na number of years. They come back here to get my voice recorded and to get Bill’s [Gates] voice\\nand Charlie’s voice. They do it all themselves just to participate in the movie. Andy and I — I’m working with Andy on a cartoon series that will be out pretty soon, which\\nwe’re aiming toward younger people to try and work a little financial education into a good\\ntime on Saturday morning for kids. And we’ll see how that all comes out. But Andy is wonderful to work with. It’s been —\\n(Applause)\\nWARREN BUFFETT: My daughter Suze puts that movie together. It’s a lot of work and it’s a\\nlabor of love. She does a terrific job, and I just want to thank her for — as usual. (Applause)\\nThank you. WARREN BUFFETT: Then finally I want to particularly thank the grand impresario of this whole\\naffair is Kelly Broz. And Kelly puts this all together, the exhibition hall. I just turn it over to her. I forget about it,\\nand Charlie and I just show up on Saturday morning. And Kelly is having her 50th birthday tomorrow. So, Kelly, would you stand up and take a bow,\\nplease. Yeah.\\n(Singing) Happy birthday to you. Happy birthday to you. Happy birthday, dear Kelly. Happy\\nbirthday to you. For Kelly. (Applause)\\nWARREN BUFFETT: Now, today we’re going to follow the usual format. We have a number of\\nmicrophones placed around this room and we have overflow rooms.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5023334f-222d-49e3-8dd1-7a656d11fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/Unprocessed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.pdf\n",
      "Error processing chunk: And we have Fred Schwed’s “Where Are the Customers’ Yachts?” book, which contains an\n",
      "incredible amou...\n",
      "Error details: Invalid \\escape: line 3 column 77 (char 219)\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Meeting Transcripts/\"\n",
    "output_directory = \"Dataset/Processed/Meeting Transcripts/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4e5786-9309-4ad8-9f3a-ccc59953ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(input_directory, output_path):\n",
    "    \"\"\"\n",
    "    Merge all JSON files in the specified directory into a single JSON file\n",
    "    maintaining the nested 'conversations' structure.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing JSON files\n",
    "        output_path (str): Full path (including filename) for the output merged JSON file\n",
    "    \"\"\"\n",
    "    # Initialize the merged structure\n",
    "    merged_data = {\n",
    "        \"conversations\": []\n",
    "    }\n",
    "    \n",
    "    # Convert string paths to Path objects\n",
    "    directory = Path(input_directory)\n",
    "    output = Path(output_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all JSON files in the directory\n",
    "    for file_path in directory.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Check if the file has the expected structure\n",
    "                if \"conversations\" in data:\n",
    "                    # Extend the conversations list with the new data\n",
    "                    merged_data[\"conversations\"].extend(data[\"conversations\"])\n",
    "                else:\n",
    "                    print(f\"Warning: File {file_path} does not have the expected structure\")\n",
    "                    \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not parse JSON from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Write the merged data to the specified output path\n",
    "    try:\n",
    "        with open(output, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Successfully created merged file at: {output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing merged file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59768dad-6f7f-4589-aa96-d272cb85e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created merged file at: Dataset/Processed/Shareholder Letters/Letters.json\n"
     ]
    }
   ],
   "source": [
    "merge_json_files('Dataset/Processed/Shareholder Letters/', 'Dataset/Processed/Shareholder Letters/Letters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa357e9-7ea0-4242-ac29-ee786648bb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset/Processed/Merged/Transcripts.json'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy2('Dataset/Processed/Shareholder Letters/Letters.json', 'Dataset/Processed/Ground Truth/Letters.json')\n",
    "shutil.copy2('Dataset/Processed/Lessons for Corporate America/Lessons-for-Corporate-America.json', 'Dataset/Processed/Ground Truth/Lessons.json')\n",
    "shutil.copy2('Dataset/Processed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.json', 'Dataset/Processed/Ground Truth/Transcripts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f9eb351-e8ef-4262-872a-78860c69c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created merged file at: Dataset/Processed/Ground Truth/dataset_combined.json\n"
     ]
    }
   ],
   "source": [
    "merge_json_files('Dataset/Processed/Ground Truth/', 'Dataset/Processed/Ground Truth/dataset_combined.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9c927a-0481-4ecc-bca4-959e706e7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_conversations(input_file, output_file):\n",
    "    # Read the input JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # New list to store split conversations\n",
    "    new_conversations = []\n",
    "    \n",
    "    # Process each conversation group\n",
    "    for conv_group in data['conversations']:\n",
    "        # Get the inner conversations list\n",
    "        inner_convs = conv_group[0]['conversations']\n",
    "        \n",
    "        # Process pairs of messages\n",
    "        for i in range(0, len(inner_convs), 2):\n",
    "            if i + 1 < len(inner_convs):  # Make sure we have a pair\n",
    "                # Create a new conversation group with just this pair\n",
    "                new_group = [{\n",
    "                    \"conversations\": [\n",
    "                        inner_convs[i],     # human message\n",
    "                        inner_convs[i + 1]  # assistant message\n",
    "                    ]\n",
    "                }]\n",
    "                new_conversations.append(new_group)\n",
    "    \n",
    "    # Create new JSON structure\n",
    "    new_data = {\n",
    "        \"conversations\": new_conversations\n",
    "    }\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b9a5e1-fba3-4c25-ab41-9a05ad50b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/notebooks/AlphaBuffet/Dataset/Processed/Ground Truth/dataset_combined.json'\n",
    "output_file = '/notebooks/AlphaBuffet/Dataset/Processed/Ground Truth/dataset_combined_2.json'\n",
    "split_conversations(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c87f08-63c3-462f-951a-5a160b4525a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
