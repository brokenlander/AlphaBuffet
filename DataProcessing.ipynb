{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "272f88a8-c392-44ed-af7b-38a27aeb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import spacy\n",
    "import lxml.html\n",
    "import re\n",
    "import fitz\n",
    "from openai import OpenAI, Client\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d96fe8c8-6a13-4fa8-9e92-0496c673646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    \"\"\"Represents a single sentence with its text and length.\"\"\"\n",
    "    text: str\n",
    "    length: int\n",
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking and processing.\"\"\"\n",
    "    max_chunk_size: int = 1500\n",
    "    min_chunk_size: int = 500\n",
    "    overlap_sentences: int = 2\n",
    "    \n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Client, config: ChunkConfig):\n",
    "        \"\"\"Initialize the document processor with improved chunking capabilities.\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['tagger', 'ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text before processing.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        # Normalize line endings\n",
    "        text = text.replace('\\n', ' ')\n",
    "        return text\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks with improved handling.\"\"\"\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        # Preprocess the extracted text\n",
    "        clean_text = self.preprocess_text(text)\n",
    "        return self._chunk_text(clean_text)\n",
    "\n",
    "\n",
    "    def process_txt(self, txt_path: str) -> List[str]:\n",
    "        \"\"\"Read text file and split into chunks.\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Preprocess the text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            return self._chunk_text(clean_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text file {txt_path}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def create_sentence_objects(self, doc) -> List[Sentence]:\n",
    "        \"\"\"Convert spaCy doc into list of Sentence objects.\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            text = sent.text.strip()\n",
    "            if text:  # Only include non-empty sentences\n",
    "                sentences.append(Sentence(text=text, length=len(text)))\n",
    "        return sentences\n",
    "\n",
    "    def _chunk_sentences(self, sentences: List[Sentence]) -> List[List[Sentence]]:\n",
    "        \"\"\"Split sentences into chunks while maintaining proper overlap.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # Always add the current sentence\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence.length\n",
    "            \n",
    "            # Check if we should create a new chunk\n",
    "            if current_length >= self.config.max_chunk_size and len(current_chunk) > self.config.overlap_sentences:\n",
    "                # Only create chunk if it meets minimum size\n",
    "                if current_length >= self.config.min_chunk_size:\n",
    "                    chunks.append(current_chunk)\n",
    "                    \n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_sentences = current_chunk[-self.config.overlap_sentences:]\n",
    "                    current_chunk = overlap_sentences.copy()\n",
    "                    current_length = sum(s.length for s in current_chunk)\n",
    "\n",
    "        # Add the last chunk if it meets minimum size\n",
    "        if current_length >= self.config.min_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Enhanced text chunking with better overlap handling.\"\"\"\n",
    "        # Create spaCy doc and convert to sentence objects\n",
    "        doc = self.nlp(text)\n",
    "        sentences = self.create_sentence_objects(doc)\n",
    "        \n",
    "        # Create chunks of sentences\n",
    "        sentence_chunks = self._chunk_sentences(sentences)\n",
    "        \n",
    "        # Convert chunks of sentences back to text\n",
    "        text_chunks = []\n",
    "        for chunk in sentence_chunks:\n",
    "            chunk_text = ' '.join(sentence.text for sentence in chunk)\n",
    "            text_chunks.append(chunk_text)\n",
    "            \n",
    "        return text_chunks\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate conversation from chunk with validation.\"\"\"\n",
    "        # Validation prompt remains unchanged\n",
    "        validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "- Buffett does not explicitly state a broad lesson. The text must include a clear, stated takeaway that can apply to other cases.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return only \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise.\n",
    "\"\"\"\n",
    "        try:\n",
    "            validation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            if validation_response.choices[0].message.content.strip().lower() != \"yes\":\n",
    "                return None\n",
    "\n",
    "            # Generate conversation prompt remains unchanged\n",
    "            conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Generate 1-2 questions that could be asked about this specific content, but ONLY if the text contains clear, direct information to answer them. Then provide my answers in first person, as if I am directly responding to these questions. Use my communication style—plain-spoken, using analogies when helpful, and occasionally humorous.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Important guidelines:\n",
    "- Only generate questions about topics that are explicitly discussed in this text excerpt\n",
    "- Write answers in first person\n",
    "- Use my direct, plain-spoken style.\n",
    "- Keep answers focused on what's actually in the text\n",
    "- Return as a JSON string in ShareGPT format:\n",
    "[{{\"conversations\": [\n",
    "    {{\"role\": \"human\", \"content\": \"question here\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"answer here\"}},\n",
    "    {{\"role\": \"human\", \"content\": \"second question\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"second answer\"}}\n",
    "]}}]\"\"\"\n",
    "\n",
    "            conversation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.4\n",
    "            )\n",
    "            \n",
    "            response_text = conversation_response.choices[0].message.content\n",
    "            return json.loads(response_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def process_directory(input_dir: str, output_dir: str, client: Client, config: ChunkConfig):\n",
    "    \"\"\"Process all PDF and TXT files in a directory and generate training data.\"\"\"\n",
    "    processor = DocumentProcessor(client, config)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Walk through directory, skipping hidden directories\n",
    "    for file_path in Path(input_dir).rglob('*'):\n",
    "        # Skip hidden directories and their contents\n",
    "        if any(part.startswith('.') for part in file_path.parts):\n",
    "            continue\n",
    "            \n",
    "        if file_path.suffix.lower() in ['.pdf', '.txt']:\n",
    "            print(f\"Processing {file_path}\")\n",
    "            output_file = Path(output_dir) / (file_path.stem + '.json')\n",
    "            \n",
    "            # Process based on file type\n",
    "            if file_path.suffix.lower() == '.pdf':\n",
    "                chunks = processor.process_pdf(str(file_path))\n",
    "            else:  # .txt file\n",
    "                chunks = processor.process_txt(str(file_path))\n",
    "            \n",
    "            file_conversations = []\n",
    "            for chunk in chunks:\n",
    "                conversation = processor.generate_conversation(chunk)\n",
    "                if conversation:\n",
    "                    file_conversations.append(conversation)\n",
    "            \n",
    "            # Save conversations for this file\n",
    "            if file_conversations:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\"conversations\": file_conversations}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "70106e46-3cda-4eb2-9359-416d87cb53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configuration\n",
    "config = ChunkConfig(\n",
    "    max_chunk_size=1500,    # Maximum characters per chunk\n",
    "    min_chunk_size=500,     # Minimum characters per chunk\n",
    "    overlap_sentences=2      # Number of sentences to overlap (changed from overlap_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "779e3261-6548-4d63-8ccf-7327c76298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Client\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://82.150.117.181:8000/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "724b6d70-4c68-43a9-941f-e57c3b26c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 406 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Lessons for Corporate America/Lessons-for-Corporate-America.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "75e504f2-b73f-474c-931c-20f281e0873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997] THE ESSAYS OF WARREN BUFFETT 15 Buffett learned the art of investing from Ben Graham as a graduate student at Columbia Business School in the 1950s and later working at Graham-Newman. In a number of classic works, including The Intelligent Investor, Graham introduced some of the most profound investment wisdom in history. It rejects a prevalent but mistaken mind-set that equates price with value. On the con- trary, Graham held that price is what you pay and value is what you get. These two things are rarely identical, but most people rarely notice any difference. One of Graham's most profound contributions is a character who lives on Wall Street, Mr. Market. He is your hypothetical business partner who is daily willing to buy your interest in a busi- ness or sell you his at prevailing market prices. Mr. Market is moody, prone to manic swings from joy to despair. Sometimes he offers prices way higher than value; sometimes he offers prices way lower than value. The more manic-depressive he is, the greater the spread between price and value, and therefore the greater the in- vestment opportunities he offers. Buffett reintroduces Mr. Market, emphasizing how valuable Graham's allegory of the overall market is for disciplined investment knitting-even though Mr. Market would be unrecognizable to modern finance theorists. Another leading prudential legacy from Graham is his margin- of-safety principle. This principle holds that one should not make an investment in a security unless there is a sufficient basis for be- lieving that the price being paid is substantially lower than the value being delivered.\n"
     ]
    }
   ],
   "source": [
    "print(chunks[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "192517d8-7192-40ba-ada6-1ddb5bb704ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample conversation:\n",
      "[\n",
      "  {\n",
      "    \"conversations\": [\n",
      "      {\n",
      "        \"role\": \"human\",\n",
      "        \"content\": \"What is the main difference between price and value according to Ben Graham's investment wisdom?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Well, as I learned from Ben, price is what you pay, and value is what you get. It's like buying a sandwich - the price is what's on the menu, but the value is how full you are after eating it. They're not always the same thing, and that's where the opportunities lie.\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"human\",\n",
      "        \"content\": \"Can you explain the concept of Mr. Market and how it relates to investment opportunities?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Mr. Market is a great allegory that Ben Graham came up with. He's like a moody business partner who's always willing to buy or sell at the current market price. The thing is, he's prone to wild mood swings, so sometimes he'll offer you a great deal, and sometimes he'll try to rip you off. The more erratic he is, the bigger the difference between price and value, and that's when the real investment opportunities pop up.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    conversation = processor.generate_conversation(chunks[23])\n",
    "    print(\"Sample conversation:\")\n",
    "    print(json.dumps(conversation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f229c-917f-448e-8db9-6c6075a3a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Shareholder Letters/\"\n",
    "output_directory = \"Dataset/Processed/Shareholder Letters/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90983a70-f5cb-4139-9d69-20e3322a4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Shareholder Letters/Lessons for Corporate America/\"\n",
    "output_directory = \"Dataset/Processed/Shareholder Letters/Lessons for Corporate America/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "818df5f2-e08e-446a-b0ad-165b82f177fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    \"\"\"Represents a single sentence with its text and length.\"\"\"\n",
    "    text: str\n",
    "    length: int\n",
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking and processing.\"\"\"\n",
    "    max_chunk_size: int = 1600\n",
    "    overlap_sentences: int = 1\n",
    "    \n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Client, config: ChunkConfig):\n",
    "        \"\"\"Initialize the document processor.\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['tagger', 'ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text while preserving important newlines.\"\"\"\n",
    "        # Split into lines\n",
    "        lines = text.split('\\n')\n",
    "        # Remove empty lines and excessive whitespace within lines\n",
    "        lines = [' '.join(line.split()) for line in lines if line.strip()]\n",
    "        # Rejoin with newlines\n",
    "        return '\\n'.join(lines)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks.\"\"\"\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        # Preprocess the extracted text\n",
    "        clean_text = self.preprocess_text(text)\n",
    "        return self._chunk_text(clean_text)\n",
    "\n",
    "    def process_txt(self, txt_path: str) -> List[str]:\n",
    "        \"\"\"Read text file and split into chunks.\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Preprocess the text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            return self._chunk_text(clean_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text file {txt_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _split_large_chunk(self, text: str) -> List[str]:\n",
    "        \"\"\"Split a large chunk of text into roughly equal parts at sentence boundaries.\"\"\"\n",
    "        # Use spaCy to split into sentences\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return [text]\n",
    "            \n",
    "        # Calculate target split point\n",
    "        total_length = len(text)\n",
    "        target_length = total_length // 2\n",
    "        \n",
    "        # Find best split point\n",
    "        current_length = 0\n",
    "        best_split_idx = 0\n",
    "        \n",
    "        for i, sent in enumerate(sentences):\n",
    "            current_length += len(sent.text)\n",
    "            if current_length >= target_length:\n",
    "                best_split_idx = i\n",
    "                break\n",
    "        \n",
    "        # Create the two chunks\n",
    "        first_chunk = ' '.join(sent.text.strip() for sent in sentences[:best_split_idx + 1])\n",
    "        second_chunk = ' '.join(sent.text.strip() for sent in sentences[best_split_idx + 1:])\n",
    "        \n",
    "        return [first_chunk, second_chunk]\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks based on numbered sections and size limits.\"\"\"\n",
    "        # Split text into lines\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        initial_chunks = []\n",
    "        current_chunk = []\n",
    "        \n",
    "        # First split by numbered sections\n",
    "        for line in lines:\n",
    "            # Check if line starts with a number followed by a dot\n",
    "            if re.match(r'^\\d+\\.', line.strip()):\n",
    "                # If we have accumulated lines in the current chunk, save it\n",
    "                if current_chunk:\n",
    "                    initial_chunks.append('\\n'.join(current_chunk))\n",
    "                # Start a new chunk with the current line\n",
    "                current_chunk = [line]\n",
    "            else:\n",
    "                # Add line to current chunk\n",
    "                current_chunk.append(line)\n",
    "        \n",
    "        # Add the last chunk if it exists\n",
    "        if current_chunk:\n",
    "            initial_chunks.append('\\n'.join(current_chunk))\n",
    "        \n",
    "        # Further split chunks that are too large\n",
    "        final_chunks = []\n",
    "        for chunk in initial_chunks:\n",
    "            if len(chunk) > self.config.max_chunk_size:\n",
    "                split_chunks = self._split_large_chunk(chunk)\n",
    "                final_chunks.extend(split_chunks)\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "            \n",
    "        return final_chunks\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate conversation from chunk with validation.\"\"\"\n",
    "        # Validation prompt remains unchanged\n",
    "        validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "- Buffett does not explicitly state a broad lesson. The text must include a clear, stated takeaway that can apply to other cases.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return only \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise.\n",
    "\"\"\"\n",
    "        try:\n",
    "            validation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            if validation_response.choices[0].message.content.strip().lower() != \"yes\":\n",
    "                return None\n",
    "\n",
    "            conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Generate 1-2 questions that could be asked about this specific content, but ONLY if the text contains clear, direct information to answer them. Then provide my answers in first person, as if I am directly responding to these questions. Use my communication style—plain-spoken, using analogies when helpful, and occasionally humorous.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Important guidelines:\n",
    "- Only generate questions about topics that are explicitly discussed in this text excerpt\n",
    "- Write answers in first person\n",
    "- Use my direct, plain-spoken style.\n",
    "- Keep answers focused on what's actually in the text\n",
    "- Return as a JSON string in ShareGPT format:\n",
    "[{{\"conversations\": [\n",
    "    {{\"role\": \"human\", \"content\": \"question here\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"answer here\"}},\n",
    "    {{\"role\": \"human\", \"content\": \"second question\"}},\n",
    "    {{\"role\": \"assistant\", \"content\": \"second answer\"}}\n",
    "]}}]\"\"\"\n",
    "\n",
    "            conversation_response = self.client.chat.completions.create(\n",
    "                model=\"/model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.4\n",
    "            )\n",
    "            \n",
    "            response_text = conversation_response.choices[0].message.content\n",
    "            return json.loads(response_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1d4ff69a-a12e-4f65-a3b9-b84614ba0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ChunkConfig()\n",
    "processor = DocumentProcessor(client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "37969412-d94f-4d07-8be0-6c8427db5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3796\n"
     ]
    }
   ],
   "source": [
    "# Process single PDF\n",
    "pdf_path = \"Dataset/Unprocessed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.pdf\"\n",
    "chunks = processor.process_pdf(pdf_path)\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "72b91761-f136-48eb-b0bc-0156f00b6001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19. “A complicated bankruptcy can offer opportunities for profit”\\nWARREN BUFFETT: Number 2.\\nAUDIENCE MEMBER: Hello. My name is Barry Steinhart (PH), shareholder from New York. My\\nquestion relates to the Chapter 11 bankruptcy process. I know you have been active in the past in some activity in the bankruptcy court. And if you had\\nthoughts on possible reforms in that area, if you believe that any reforms are necessary?\\nWARREN BUFFETT: Well, that’s a good question. Charlie is probably better qualified to answer\\nthan I am. I mean, we have bought Fruit of the Loom out of bankruptcy. And we have had some involvement in owning junk bonds. You know, we get — we think about\\nthe bankruptcy process. But in terms of the practicalities of improving on it, what do you think,\\nCharlie?\\nCHARLIE MUNGER: Well, I think much of that is pretty horrible. You have a competition there, where the courts themselves have gone into bidding contests to\\nget bankruptcy business attracted. Meaning that the —\\nThere are various courts that can handle bankruptcy cases. And they have found that if they\\ndevelop a culture where they overpay a lot of people egregiously, they can attract more\\nbusiness: lawyers, trustees, consultants, et cetera, et cetera. I find it so unpleasant to watch that I don’t pay as much attention to bankruptcy as I probably\\nshould. You know, I’m an old man, and I don’t like to have an upset stomach. (Laughter)\\nWARREN BUFFETT: But we will — we look at — at least I do, I’m not sure about Charlie — but\\nhe — you know, bankruptcies will be something that we will — one way or another, over the\\nnext 20 years — we’ll have various ways of participating. And we have bought — well, we bought certain of the bonds, for example, of Enron after they\\nentered bankruptcy — we bought something called the Ospreys. And a complicated bankruptcy can offer opportunities for profit. Now, there’s so many people\\nlooking at bankruptcies currently, or potential bankruptcies, it’s a field that I would say does\\nnot have a lot of promise right now, but it has had promise in the past. We actually, in the Fruit of the Loom situation, I first went into that just by buying some Fruit of\\nthe Loom bonds, but — when I had no notion that we might conceivably end up with the\\ncompany. But, you know, I knew enough about it to buy the bonds. And Enron comes along, and it’s a big mess. The Penn Central came along 20-odd years ago,\\nand it was a big mess, and there was a lot of money made in the Penn Central, simply because it\\nwas such a complicated mess. So anytime there’s something big, complicated, there’s certainly a good chance of mispricing. Now, lately the mispricing may be more on the high side than on the low side. But, over time,\\nyou’re going to find some — there will be some attractive things to do in bankruptcy situations.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023334f-222d-49e3-8dd1-7a656d11fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Meeting Transcripts/\"\n",
    "output_directory = \"Dataset/Processed/Meeting Transcripts/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
